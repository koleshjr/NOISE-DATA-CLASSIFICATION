{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Noise_data_simple_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVpLPol2+oWowJJSoQ6hVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koleshjr/NOISE-DATA-CLASSIFICATION/blob/main/Noise_data_simple_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JHbi-Ox6z4C"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Noise\" -r \"/content/\""
      ],
      "metadata": {
        "id": "on9Cipr07LMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from glob import glob ## list all files \n",
        "\n",
        "##working with audio\n",
        "import librosa\n",
        "import librosa.display \n",
        "import IPython.display as ipd ## play audio in the notebook\n",
        "\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "## pretty visualizations\n",
        "sns.set_theme(style=\"white\", palette= None)\n",
        "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
        "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "v8TdzEyo7_bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PWwF8z6t8ZvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Noise/Noise Classification/'"
      ],
      "metadata": {
        "id": "vFkJggRa7p0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(path + \"Train.csv\")\n",
        "test = pd.read_csv(path + \"Test.csv\")\n",
        "ss = pd.read_csv(path + \"SampleSubmission.csv\")"
      ],
      "metadata": {
        "id": "WKFJCwOT7gxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "VOoE6BYb78_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip '/content/drive/MyDrive/Noise/Noise Classification/Audio_clips.zip'"
      ],
      "metadata": {
        "id": "z-E1dW4h8U1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FREQUENCY\n",
        "* Differences in audio wavelengths\n",
        "* Every audio has multiple frequencies within it \n",
        "* we have low, high and medium frequencies \n",
        "\n",
        "### INTENSITY(POWER)\n",
        "*  Changes in the height of the wave(loudness, pitch)\n",
        "\n",
        "### Sample Rate\n",
        "* Discrete observations viewed by the computer \n",
        "* Resolution of the audio\n",
        "* Specific to how the computer reads in the audio file "
      ],
      "metadata": {
        "id": "3EGQvqeX9UjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files = glob(\"/content/AUDIO_CLIPS/*.wav\")"
      ],
      "metadata": {
        "id": "PL0lrrgs96c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play Audio File"
      ],
      "metadata": {
        "id": "sphWDej1Afss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['category'].value_counts()"
      ],
      "metadata": {
        "id": "IkURPQF1AzsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(audio_files[500])"
      ],
      "metadata": {
        "id": "J-50mJqyAPzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, sr = librosa.load(audio_files[5])\n",
        "print(f'y:{y[:10]}')\n",
        "print(f'shape y:{y.shape}')\n",
        "print(f'sr:{sr}')"
      ],
      "metadata": {
        "id": "Tq2LiecoBT8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y).plot(figsize=(10,5), title='Raw Audio Example',lw=1,color = color_pal[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kMPbqDaQBT_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
        "# pd.Series(y_trimmed).plot(figsize=(10,5), title='Raw Audio Example',lw=1,color = color_pal[1])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "PoMVIAcwBUGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y[150000:155000]).plot(figsize=(10,5), title='Raw Audio Zoomed in Example',lw=1,color = color_pal[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wOK3OQbpBUI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectogram\n",
        "* a detailed view of audio, able to represent time, frequency, and amplitude all on one graph\n",
        "* Ideally suited for applications where all frequencies have equal importance"
      ],
      "metadata": {
        "id": "MvB_sWVvEYeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = librosa.stft(y)\n",
        "s_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "s_db.shape"
      ],
      "metadata": {
        "id": "RrjAEyZXAdIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the transformed audio data\n",
        "fig, ax = plt.subplots(figsize=(18, 5))\n",
        "img= librosa.display.specshow(s_db, x_axis = 'time',y_axis = 'log', ax=ax)\n",
        "ax.set_title('Spectogram Example', fontsize=28)\n",
        "fig.colorbar(img, ax=ax, format = f'%0.2f')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0yWKdqME8Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MEL SPECTOGRAM\n",
        "* remaps the frequency in Hz to the mel_scale\n",
        "* better suited for applications that need to model human hearing perception"
      ],
      "metadata": {
        "id": "LRnkY0N9GCLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = librosa.feature.melspectrogram(y,sr=sr,n_mels=64)\n",
        "s_db_mel= librosa.amplitude_to_db(S, ref=np.max)\n",
        "s_db_mel.shape"
      ],
      "metadata": {
        "id": "VOrbVnBkFYuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the transformed audio data\n",
        "fig, ax = plt.subplots(figsize=(18, 5))\n",
        "img= librosa.display.specshow(s_db_mel, x_axis = 'time',y_axis = 'log', ax=ax)\n",
        "ax.set_title('Mel Spectogram Example', fontsize=28)\n",
        "fig.colorbar(img, ax=ax, format = f'%0.2f')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UMifxbmIGRxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zDAUJiJQGeNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PYTORCH CUSTOM DATASET IN AUDIO CLASSIFICATION"
      ],
      "metadata": {
        "id": "kvzgsTFqLN-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create_folds for the model\n",
        "## add .wav extension to the audio_id to match the audio"
      ],
      "metadata": {
        "id": "xVaAHXzIMtUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "Fq5GGfG5RDZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "df = train.copy()\n",
        "encoder = LabelEncoder()\n",
        "df['Label'] = encoder.fit_transform(df['category'])\n",
        "df = df.reset_index()\n",
        "df.drop(df[df.index %2 != 0].index, inplace = True)\n",
        "df =df[['CLIP_ID','category','Label']].reset_index(drop = True)\n",
        "df"
      ],
      "metadata": {
        "id": "x7S6iq4AQ5qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state=42)\n",
        "for n,(_,valid_index)  in enumerate(kf.split(df,df['Label'])):\n",
        "  df.loc[valid_index,'fold'] = int(n)\n",
        "df['fold'] = df['fold'].astype(int)"
      ],
      "metadata": {
        "id": "rslHavS5RTpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['fold']==0][df[df['fold']==0]['Label']== 2]"
      ],
      "metadata": {
        "id": "16yQNqJHRo31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"CLIP_ID\"] = [i +\".wav\" for i in df[\"CLIP_ID\"]]"
      ],
      "metadata": {
        "id": "y5O-QE7mR37J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "hI2M0kE1UsAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/train_10folds.csv', index=False)"
      ],
      "metadata": {
        "id": "y9GdqH7jR9DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train_10folds.csv')"
      ],
      "metadata": {
        "id": "TDIyJnFBSXAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "pn-6FGIuN1uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "RzkF6gTlTQNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoiseDataset(Dataset):\n",
        "  def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples, device ):\n",
        "    self.annotations = pd.read_csv(annotations_file)\n",
        "    self.audio_dir = audio_dir\n",
        "    self.device = device\n",
        "    self.transformation = transformation.to(self.device)\n",
        "    self.target_sample_rate = target_sample_rate\n",
        "    self.num_samples = num_samples\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    ##we are using private methods \n",
        "    audio_sample_path = self._get_audio_sample_path(index)\n",
        "    label = self._get_audio_sample_label(index)\n",
        "    signal, sr= torchaudio.load(audio_sample_path)\n",
        "\n",
        "    signal = signal.to(self.device) ## transforms on gpu if there \n",
        "    signal = self._resample_if_necessary(signal,sr) # each audio has diff so uniform\n",
        "    signal = self._mix_down_if_necessary(signal) ##multiple channels to one channel\n",
        "    signal = self._right_pad_if_necessary(signal)\n",
        "    signal = self._cut_if_necessary(signal)\n",
        "    signal = self.transformation(signal)\n",
        "    return signal, label\n",
        "\n",
        "  def _cut_if_necessary(self, signal):\n",
        "    # signal -> Tensor ->(1, num_samples)\n",
        "    if signal.shape[1] > self.num_samples:\n",
        "      signal = signal[:, :self.num_samples]\n",
        "    return signal\n",
        "\n",
        "  def _right_pad_if_necessary(self,signal):\n",
        "    len_signal = signal.shape[1]\n",
        "    if len_signal < self.num_samples:\n",
        "      num_missing_samples = self.num_samples - len_signal\n",
        "      last_dim_padding = (0, num_missing_samples)\n",
        "      signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "    return signal\n",
        "  def _resample_if_necessary(self,signal,sr):\n",
        "    if sr!= self.target_sample_rate:## we dont want to resample when sr is equal to target sr\n",
        "      resampler = torchaudio.transforms.Resample(sr,self.target_sample_rate)\n",
        "      signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "  def _mix_down_if_necessary(self, signal):\n",
        "    if signal.shape[0] > 1: # (2, 1000)\n",
        "      signal = torch.mean(signal, dim=0, keepdim= True)\n",
        "    return signal\n",
        "\n",
        "  def _get_audio_sample_path(self, index):\n",
        "    path = os.path.join(self.audio_dir,self.annotations.iloc[index,0])\n",
        "    return path\n",
        "\n",
        "  def _get_audio_sample_label(self, index):\n",
        "    ##pass the index with the label_encoded_class\n",
        "    return self.annotations.iloc[index, 2]\n",
        "\n"
      ],
      "metadata": {
        "id": "F6OxlalQLY9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATIONS_FILE = '/content/train_10folds.csv'\n",
        "AUDIO_DIR = '/content/AUDIO_CLIPS'\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device=\"cuda\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(f\"running_device is {device}\")\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate= SAMPLE_RATE,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels = 64\n",
        ")\n",
        "\n",
        "##\n",
        "noise_data = NoiseDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE,\n",
        "                          NUM_SAMPLES,device)"
      ],
      "metadata": {
        "id": "nuvH78WZQYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"there are {len(noise_data)} clips in the dataset\")"
      ],
      "metadata": {
        "id": "_aY3IynTS0dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_data[1]"
      ],
      "metadata": {
        "id": "b242ntmuTZhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Everything works okay"
      ],
      "metadata": {
        "id": "BTw2B5bbV15E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL CREATION"
      ],
      "metadata": {
        "id": "yWyvpGljvsod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Change this to a dynamicc way by using the constructor"
      ],
      "metadata": {
        "id": "kxP27ixMykCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    # 4 conv_blocks/ flatten/ linear/ softmax\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=1, ##assume it to  be gray_scale\n",
        "            out_channels=16,\n",
        "            kernel_size =3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, ##assume it to  be gray_scale\n",
        "            out_channels=32,\n",
        "            kernel_size =3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=32, ##assume it to  be gray_scale\n",
        "            out_channels=64,\n",
        "            kernel_size =3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=64, ##assume it to  be gray_scale\n",
        "            out_channels=128,\n",
        "            kernel_size =3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.flatten = nn.Flatten()\n",
        "    #128 no of last block output channel\n",
        "    self.linear = nn.Linear(128*5*4, 19)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, input_data):\n",
        "      x = self.conv1(input_data)\n",
        "      x = self.conv2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv4(x)\n",
        "      x= self.flatten(x)\n",
        "      logits = self.linear(x)\n",
        "      predictions = self.softmax(logits)\n",
        "      return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "zHzFSqH1VHEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = MyModel()\n",
        "## 64 mel_specs, 44 time_spec\n",
        "summary(cnn.to(device), (1,64,44))"
      ],
      "metadata": {
        "id": "G_hFGabJxKtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "JwVSsNbA4EAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "Q7ddNcvmzHiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(train_data,batch_size):\n",
        "  train_dataloader = DataLoader(train_data, batch_size = batch_size)\n",
        "  return train_dataloader\n",
        "\n",
        "def train_single_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "  for input, target in data_loader:\n",
        "    input, target = input.to(device), target.to(device)\n",
        "\n",
        "    #calculate loss\n",
        "    prediction = model(input)\n",
        "    loss = loss_fn(prediction, target)\n",
        "\n",
        "    ##backpropagate error and update weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"loss: {loss.item()}\")\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
        "  for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\")\n",
        "    train_single_epoch(model, data_loader, loss_fn, optimizer, device)\n",
        "    print(\"-----------------------------------------------------\")\n",
        "  print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "Q5NrSzAt4oCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiate our dataset object\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate= SAMPLE_RATE,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels = 64\n",
        ")\n",
        "\n",
        "##\n",
        "noise_data = NoiseDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE,\n",
        "                          NUM_SAMPLES,device)\n",
        "\n",
        "train_dataloader = create_data_loader(noise_data, BATCH_SIZE)\n",
        " \n",
        "## Construct model and assign it to device\n",
        "my_model = MyModel().to(device)\n",
        "print(my_model)\n",
        "\n",
        "## Initialise the loss_funtion and optimiser\n",
        "loss_fn = nn. CrossEntropyLoss()\n",
        "optimiser = torch. optim.Adam(my_model.parameters(),\n",
        "                              lr = LEARNING_RATE)\n",
        "\n",
        "## Train the model\n",
        "train(my_model, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n",
        "\n",
        "## Save the model\n",
        "torch.save(my_model.state_dict(), \"baseline_scratch_model.pth\")\n",
        "print(\"Trained model saved at baseline_scratch_model.pth\")\n"
      ],
      "metadata": {
        "id": "Is9CHowI6L9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xyKeEawR8q9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}